{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDS Assignment 1: UN Debates, World Happiness and International Trade\n",
    "\n",
    "We can start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO-alpha3 Code</th>\n",
       "      <th>Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>WSM</td>\n",
       "      <td>Mr. President,\\nDistinguished delegates,\\nLadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8477</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>YEM</td>\n",
       "      <td>In the name of God the Merciful and the Compas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZAF</td>\n",
       "      <td>President of the General Assembly, Secretary-G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>Your excellency Mr. Volkan Bozkir, President o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Your Excellency, Ambassador Volkan Bozkir, Pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Session  Year ISO-alpha3 Code  \\\n",
       "8476       75  2020             WSM   \n",
       "8477       75  2020             YEM   \n",
       "8478       75  2020             ZAF   \n",
       "8479       75  2020             ZMB   \n",
       "8480       75  2020             ZWE   \n",
       "\n",
       "                                                 Speech  \n",
       "8476  Mr. President,\\nDistinguished delegates,\\nLadi...  \n",
       "8477  In the name of God the Merciful and the Compas...  \n",
       "8478  President of the General Assembly, Secretary-G...  \n",
       "8479  Your excellency Mr. Volkan Bozkir, President o...  \n",
       "8480  Your Excellency, Ambassador Volkan Bozkir, Pre...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sessions = np.arange(25, 76)\n",
    "data=[]\n",
    "\n",
    "for session in sessions:\n",
    "    directory = \"./TXT/Session \" + str(session) + \" - \" + str(1945 + session)\n",
    "    for filename in os.listdir(directory):\n",
    "        f = open(os.path.join(directory, filename), encoding='utf8')\n",
    "        if filename[0] == \".\":\n",
    "            continue\n",
    "        splt = filename.split(\"_\")\n",
    "        data.append([session, 1945 + session, splt[0], f.read()])\n",
    "\n",
    "df_speech = pd.DataFrame(data, columns=['Session','Year','ISO-alpha3 Code','Speech'])\n",
    "df_speech.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\agniv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agniv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\agniv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(speech):\n",
    "    #tolower\n",
    "    speech = speech.lower()\n",
    "    \n",
    "    #stopwords\n",
    "    text = word_tokenize(speech)\n",
    "    tokens_without_sw = [word for word in text if not word in stopwords.words(\"english\")] \n",
    "    processed_speech = (\" \").join(tokens_without_sw)\n",
    "    \n",
    "    #punctuation, indents etc\n",
    "    processed_speech = processed_speech.translate(str.maketrans('', '', string.punctuation))\n",
    "    processed_speech = processed_speech.replace(\"\\n\",\" \")\n",
    "    processed_speech = processed_speech.replace(\"\\t\",\" \")\n",
    "    processed_speech = re.sub('\\s*\\d+\\s*', ' ', processed_speech)\n",
    "    \n",
    "    return processed_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_processing(speeches):\n",
    "    total_speech = ''\n",
    "    for speech in speeches:\n",
    "        proc_speech = preprocess(speech)\n",
    "        total_speech += (' ' + proc_speech)\n",
    "    \n",
    "    return total_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(all_years):\n",
    "    corpus = []\n",
    "#     for year in all_years:\n",
    "    for i in range(1970, 2021):\n",
    "        print(i)\n",
    "        speeches = yearly_processing(all_years.loc[i]['Speech'])\n",
    "        corpus.append(speeches)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n"
     ]
    }
   ],
   "source": [
    "corpus = iteration(df_speech.set_index([\"Year\", \"ISO-alpha3 Code\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "*Tf* means term-frequency while *tf-idf* means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document (as in the previous example) is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. If needed, more info can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1    2    3    4    5    6    7    8    9   ...   41   42   43  \\\n",
      "aa       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "aaa      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "aac      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "aachen   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "aaf      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "шг       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "ьо       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "қарекет  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "қылмақ   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "ﬂagrant  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "               44   45        46   47        48   49        50  \n",
      "aa       0.000000  0.0  0.000477  0.0  0.000000  0.0  0.000834  \n",
      "aaa      0.000000  0.0  0.000000  0.0  0.001032  0.0  0.000000  \n",
      "aac      0.000506  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
      "aachen   0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
      "aaf      0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
      "...           ...  ...       ...  ...       ...  ...       ...  \n",
      "шг       0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
      "ьо       0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000000  \n",
      "қарекет  0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000498  \n",
      "қылмақ   0.000000  0.0  0.000000  0.0  0.000000  0.0  0.000498  \n",
      "ﬂagrant  0.000000  0.0  0.000000  0.0  0.000516  0.0  0.000000  \n",
      "\n",
      "[67035 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "tf_idf = pd.DataFrame(denselist, columns=feature_names).T\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.to_csv('tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word bank\n",
    "\n",
    "List of good words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\n",
    "    'climate', 'change', 'glaciers', 'temperature', 'melting', 'greenhouse', 'fossil', 'fuels', \n",
    "    'emissions', 'co2', 'disaster', 'hurricane', 'floods', 'pollution', 'tsunami', 'drought', 'carbon', \n",
    "    'dioxide', 'carbondioxide', 'ozone', 'ozonelayer', 'global', 'warming', 'atmosphere', 'environment', \n",
    "    'environmental', 'oil', 'barrel', 'crude', 'coal', 'sea', 'level', 'ecology', 'climatologists', 'climatology', \n",
    "    'manmade', 'planet', 'earth', 'catastrophe', 'urbanization', 'terrestrial', 'antarctica', 'ice', 'depletion', \n",
    "    'nonrenewable', 'natural', 'nature', 'tree', 'deforestation', 'amazon', 'rainforest', 'forest', 'fracking', \n",
    "    'drilling', 'methane', 'leak', 'waste', 'contamination'\n",
    "]\n",
    "\n",
    "responses = [\n",
    "    'renewable', 'green', 'solar', 'clean', 'energy', 'sustainable', 'sustainability', 'carbonneutral', \n",
    "    'carbon', 'reduction', 'turbine', 'geothermal', 'hydroelectric', 'hydro', 'electricity', 'nuclear', \n",
    "    'powerplant', 'shale', ... \n",
    "Power\n",
    "Biodegradable\n",
    "Biodiversity \n",
    "Panels\n",
    "Farm(s)\n",
    "Dam(s)\n",
    "Alternative \n",
    "Ecosystem\n",
    "Ecological \n",
    "Ecofriendly \n",
    "Footprint \n",
    "Phaseout \n",
    "Decommissioning \n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
